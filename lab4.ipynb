{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuhanwwu/iml_labs/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 4: Simple Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-02-03 | Josiah Wang | First version |\n",
        "2021-11-13 | Josiah Wang | Optimisation with derivatives: $x$ should be $x^{(i)}$ |\n",
        "2022-11-02 | Josiah Wang | Updated to `ax = fig.add_subplot(projection='3d')`. The original `fig.gca(projection='3d')` is deprecated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab exercise is for you to gain some experience implementing and training a simple linear regression model from scratch. This will help you improve your understanding of linear regression and machine learning optimisation.\n",
        "\n",
        "By the end of this lab exercise, you will have\n",
        "- implemented a simple linear regression model\n",
        "- defined and implemented a loss function\n",
        "- optimised the parameters of your model using gradient descent\n",
        "\n",
        "There will be a bit less coding required on your side in this exercise compared to previous exercises. The aim is for you to try to really understand linear regression at implementation level to complement the lectures, which will help you in future weeks as you move on to Neural Networks in your coursework assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0iaMjnIWEpe"
      },
      "source": [
        "## Simple Linear Regression\n",
        "\n",
        "In this tutorial, we will focus on the **regression task**. For simplicity, we will implement a *simple linear regression* model with one input variable and one output variable. More specifically, our task is to predict the value of $y$ given the input $x$.\n",
        "\n",
        "Let us develop our simple linear regressor with a simple toy example to make sure that our model works correctly. You can later apply it to a bigger dataset if desired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w2N6AZsYJUd",
        "outputId": "fbe094f3-ca3f-40fe-9762-31be48554ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# toy dataset\n",
        "x_train = np.array([1.0, 1.2, 2.0, 3.5, 4.0, 5.0])\n",
        "y_train = np.array([3.1, 3.5, 5.0, 7.9, 9.1, 10.9])\n",
        "x_test = np.array([2.5, 3.0, 4.5])\n",
        "y_test = np.array([6.0, 7.0, 10.1])\n",
        "\n",
        "# plot toy data\n",
        "plt.scatter(x_train, y_train, c=\"blue\")\n",
        "plt.scatter(x_test, y_test, c=\"red\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YagIkbxvaC-b"
      },
      "source": [
        "### Model\n",
        "\n",
        "As you should be able to see from the plot, the toy dataset can almost be perfectly modelled with a straight line. The model should also be able to pretty accurately predict the value of $y$ of the test set.\n",
        "\n",
        "Assuming you still remember your high school Maths, a straight line is represented as $y = wx + b$, where $w$ is the slope and $b$ the intercept/bias. Our objective is to find the line that best fits our training data. More specifically, we want our regressor to automatically learn the parameters $w$ and $b$ such that we can accurately predict the real-valued label ${\\hat y}$ given an example $x$. The objective is to get ${\\hat y}$ to be as close as possible to the values of $y$ of the training data (and presumably the true $y$).\n",
        "\n",
        "Let us now build our simple linear regression model. Complete the `forward()` method of the `SimpleLinearRegression` class below to return the value of the output $y$ given an input `x` and the current weight `w` and bias `b` of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLNpOSjXUQrY",
        "outputId": "e21506c1-3fe7-45c3-da4a-81d635efdf39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy.random import default_rng\n",
        "\n",
        "class SimpleLinearRegression:\n",
        "    def __init__(self, random_generator=default_rng()):\n",
        "        # initialise the slope with a random value drawn from a standard normal\n",
        "        # distribution (mean=0, stddev=1)\n",
        "        self.w = random_generator.standard_normal()\n",
        "\n",
        "        # initialise bias to 0\n",
        "        self.b = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Perform forward pass given an input x\n",
        "\n",
        "        Args:\n",
        "            x (float): input instance\n",
        "\n",
        "        Returns:\n",
        "            float: the output of the model given the current weights\n",
        "        \"\"\"\n",
        "        return self.w*x + self.b\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "## Quick test: This should return 8\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2\n",
        "y_hat = model.forward(x)\n",
        "print(y_hat) # should print 8"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9mtsqFXBNNa"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "From the plot and the numbers, you may have manually worked out that the 'best' line would be $\\hat{y} \\approx 2x+1$, that is, the optimal parameter values are $w \\approx 2$ and $b \\approx 1$.\n",
        "\n",
        "What constitutes the 'best' line? We will first have to define what 'best' actually means. Intuitively, the 'best' line would be the one that goes through all training points as closely as possible.\n",
        "\n",
        "To enable our model to automatically learn what the parameter values of the 'best' line are from training examples, we will have to formally define that we mean by 'best'. We define this via a *loss function* (or cost function). For this tutorial, we will use the loss function as in the lectures - the **sum of squared differences** between the predicted label vs. the ground truth label across the training instances.\n",
        "\n",
        "$$L = \\frac{1}{2} \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \\frac{1}{2} \\sum_{i=1}^{N} \\left(wx^{(i)} + b - y^{(i)}\\right)^2$$\n",
        "\n",
        "Our objective is to select the parameters $\\theta = \\{ w, b \\}$ that **minimise** the loss (or error).\n",
        "\n",
        "$$\\theta = argmin_{\\theta} \\, L$$\n",
        "\n",
        "To make things easy, let us implement the loss function for a **single** instance $x$:\n",
        "\n",
        "$$L^{(i)} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
        "\n",
        "Complete the `loss()` method of `SimpleLinearRegression` below to return the individual loss for an instance `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZM0MAgiE2UZ",
        "outputId": "0f659a35-ec77-4c45-af2b-857e6c762905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loss method for SimpleLinearRegression\n",
        "def loss(self, x, y):\n",
        "    \"\"\" Compute the loss for an input x\n",
        "\n",
        "    Args:\n",
        "        x (float): input instance\n",
        "        y (float): ground truth output\n",
        "\n",
        "    Returns:\n",
        "        float: the model loss for an instance x\n",
        "    \"\"\"\n",
        "    return (self.forward(x) - y)**2\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.loss() method\n",
        "SimpleLinearRegression.loss = loss\n",
        "\n",
        "\n",
        "## Quick test: This should return 0.25\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2.0\n",
        "y = 8.5\n",
        "test_loss = model.loss(x, y)\n",
        "print(test_loss) # should print 0.25"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxwg9EPiGu2l"
      },
      "source": [
        "### Optimisation by brute force search\n",
        "\n",
        "Now, how do we get the model to automatically figure out the optimal parameters from training data? Remember that the optimal parameter values are the ones that minimise the loss function. A naive approach would be to compute the loss for different combinations of $w$ and $b$ and selecting the combination that results in the smallest loss.\n",
        "\n",
        "The code below will search for $w$ between $0$ and $4$, and for $b$ between $0$ and $2$ to find the best combination of $w$ and $b$. Examine the code, and try to understand what it is doing, then run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyV8OCZSIdrj",
        "outputId": "57e4b654-39a6-49a1-ce61-10e665d88fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "# to store all losses for later use\n",
        "losses = []\n",
        "\n",
        "# the parameters to search\n",
        "weights = np.arange(0, 4.1, 0.2)\n",
        "biases = np.arange(0, 2.1, 0.2)\n",
        "\n",
        "# for storing the loss in a matrix for visualisation later\n",
        "loss_matrix = np.zeros((len(weights), len(biases)))\n",
        "\n",
        "# compute loss for each (w,b) combination\n",
        "for i, w in enumerate(weights):\n",
        "    for j, b in enumerate(biases):\n",
        "        print(f\"(w={w:.1f}, b={b:.1f})\")\n",
        "\n",
        "        # setup weights of model\n",
        "        model.w = w\n",
        "        model.b = b\n",
        "\n",
        "        sum_loss = 0\n",
        "        # for each example\n",
        "        for (x, y) in zip(x_train, y_train):\n",
        "            # compute the loss for this example\n",
        "            single_loss = model.loss(x, y)\n",
        "\n",
        "            # and add it to the sum\n",
        "            sum_loss += single_loss\n",
        "\n",
        "            # print out the values just to make sure everything is working correctly\n",
        "            y_hat = model.forward(x)\n",
        "            print(f\"    x: {x}, y: {y}, y_hat: {y_hat:.1f}, loss: {single_loss:.2f}\")\n",
        "\n",
        "        # print out the sum of individual losses\n",
        "        # I multiplied by 0.5 to be consistent with the equation earlier,\n",
        "        # but this is not necessary in practice as this is a constant\n",
        "        print(f\"    Loss = {(0.5 * sum_loss):.4f}\\n\")\n",
        "\n",
        "        # store the losses and the corresponding (w,b) for later use\n",
        "        losses.append((0.5*sum_loss, w, b))\n",
        "\n",
        "        # store the losses in a matrix form for visualisation later\n",
        "        loss_matrix[i,j] = 0.5 * sum_loss\n",
        "\n",
        "# find combination with minimum loss\n",
        "(min_loss, best_w, best_b) = min(losses, key=lambda x:x[0])\n",
        "print(\"BEST:\")\n",
        "print(f\"w={best_w}, b={best_b}, loss={min_loss:.4f}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(w=0.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.0, loss: 9.61\n",
            "    x: 1.2, y: 3.5, y_hat: 0.0, loss: 12.25\n",
            "    x: 2.0, y: 5.0, y_hat: 0.0, loss: 25.00\n",
            "    x: 3.5, y: 7.9, y_hat: 0.0, loss: 62.41\n",
            "    x: 4.0, y: 9.1, y_hat: 0.0, loss: 82.81\n",
            "    x: 5.0, y: 10.9, y_hat: 0.0, loss: 118.81\n",
            "    Loss = 155.4450\n",
            "\n",
            "(w=0.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
            "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.89\n",
            "    x: 2.0, y: 5.0, y_hat: 0.2, loss: 23.04\n",
            "    x: 3.5, y: 7.9, y_hat: 0.2, loss: 59.29\n",
            "    x: 4.0, y: 9.1, y_hat: 0.2, loss: 79.21\n",
            "    x: 5.0, y: 10.9, y_hat: 0.2, loss: 114.49\n",
            "    Loss = 147.6650\n",
            "\n",
            "(w=0.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.61\n",
            "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 0.4, loss: 56.25\n",
            "    x: 4.0, y: 9.1, y_hat: 0.4, loss: 75.69\n",
            "    x: 5.0, y: 10.9, y_hat: 0.4, loss: 110.25\n",
            "    Loss = 140.1250\n",
            "\n",
            "(w=0.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.41\n",
            "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 0.6, loss: 53.29\n",
            "    x: 4.0, y: 9.1, y_hat: 0.6, loss: 72.25\n",
            "    x: 5.0, y: 10.9, y_hat: 0.6, loss: 106.09\n",
            "    Loss = 132.8250\n",
            "\n",
            "(w=0.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.29\n",
            "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 0.8, loss: 50.41\n",
            "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
            "    x: 5.0, y: 10.9, y_hat: 0.8, loss: 102.01\n",
            "    Loss = 125.7650\n",
            "\n",
            "(w=0.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.25\n",
            "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 1.0, loss: 47.61\n",
            "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
            "    Loss = 118.9450\n",
            "\n",
            "(w=0.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 1.2, loss: 44.89\n",
            "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
            "    Loss = 112.3650\n",
            "\n",
            "(w=0.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
            "    Loss = 106.0250\n",
            "\n",
            "(w=0.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
            "    Loss = 99.9250\n",
            "\n",
            "(w=0.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
            "    Loss = 94.0650\n",
            "\n",
            "(w=0.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
            "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
            "    Loss = 88.4450\n",
            "\n",
            "(w=0.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
            "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.63\n",
            "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 0.7, loss: 51.84\n",
            "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
            "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
            "    Loss = 129.4688\n",
            "\n",
            "(w=0.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.36\n",
            "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 0.9, loss: 49.00\n",
            "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
            "    Loss = 122.3568\n",
            "\n",
            "(w=0.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.18\n",
            "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 1.1, loss: 46.24\n",
            "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
            "    Loss = 115.4848\n",
            "\n",
            "(w=0.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.08\n",
            "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 1.3, loss: 43.56\n",
            "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
            "    Loss = 108.8528\n",
            "\n",
            "(w=0.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.05\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 1.5, loss: 40.96\n",
            "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
            "    Loss = 102.4608\n",
            "\n",
            "(w=0.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.11\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 1.7, loss: 38.44\n",
            "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
            "    Loss = 96.3088\n",
            "\n",
            "(w=0.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 1.9, loss: 36.00\n",
            "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
            "    Loss = 90.3968\n",
            "\n",
            "(w=0.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
            "    Loss = 84.7248\n",
            "\n",
            "(w=0.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
            "    Loss = 79.2928\n",
            "\n",
            "(w=0.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
            "    Loss = 74.1008\n",
            "\n",
            "(w=0.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
            "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
            "    Loss = 69.1488\n",
            "\n",
            "(w=0.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.5, loss: 9.12\n",
            "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
            "    Loss = 105.8802\n",
            "\n",
            "(w=0.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.95\n",
            "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
            "    Loss = 99.4362\n",
            "\n",
            "(w=0.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.86\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
            "    Loss = 93.2322\n",
            "\n",
            "(w=0.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.86\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
            "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
            "    Loss = 87.2682\n",
            "\n",
            "(w=0.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.93\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 2.2, loss: 32.49\n",
            "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
            "    Loss = 81.5442\n",
            "\n",
            "(w=0.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 4.08\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 2.4, loss: 30.25\n",
            "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
            "    Loss = 76.0602\n",
            "\n",
            "(w=0.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.6, loss: 28.09\n",
            "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
            "    Loss = 70.8162\n",
            "\n",
            "(w=0.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
            "    Loss = 65.8122\n",
            "\n",
            "(w=0.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
            "    Loss = 61.0482\n",
            "\n",
            "(w=0.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
            "    Loss = 56.5242\n",
            "\n",
            "(w=0.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
            "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
            "    Loss = 52.2402\n",
            "\n",
            "(w=0.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.73\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
            "    Loss = 84.6792\n",
            "\n",
            "(w=0.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.66\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
            "    Loss = 78.9032\n",
            "\n",
            "(w=0.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.66\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
            "    Loss = 73.3672\n",
            "\n",
            "(w=0.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.75\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
            "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
            "    Loss = 68.0712\n",
            "\n",
            "(w=0.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 3.92\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.9, loss: 25.00\n",
            "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
            "    Loss = 63.0152\n",
            "\n",
            "(w=0.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.17\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.1, loss: 23.04\n",
            "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
            "    Loss = 58.1992\n",
            "\n",
            "(w=0.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.3, loss: 21.16\n",
            "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
            "    Loss = 53.6232\n",
            "\n",
            "(w=0.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
            "    Loss = 49.2872\n",
            "\n",
            "(w=0.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
            "    Loss = 45.1912\n",
            "\n",
            "(w=0.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
            "    Loss = 41.3352\n",
            "\n",
            "(w=0.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
            "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
            "    Loss = 37.7192\n",
            "\n",
            "(w=0.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.45\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
            "    Loss = 65.8658\n",
            "\n",
            "(w=0.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.48\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
            "    Loss = 60.7578\n",
            "\n",
            "(w=0.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.58\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
            "    Loss = 55.8898\n",
            "\n",
            "(w=0.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.76\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
            "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
            "    Loss = 51.2618\n",
            "\n",
            "(w=0.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 3.03\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.6, loss: 18.49\n",
            "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
            "    Loss = 46.8738\n",
            "\n",
            "(w=0.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.37\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.8, loss: 16.81\n",
            "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
            "    Loss = 42.7258\n",
            "\n",
            "(w=0.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.0, loss: 15.21\n",
            "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
            "    Loss = 38.8178\n",
            "\n",
            "(w=0.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
            "    Loss = 35.1498\n",
            "\n",
            "(w=0.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
            "    Loss = 31.7218\n",
            "\n",
            "(w=0.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
            "    Loss = 28.5338\n",
            "\n",
            "(w=0.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
            "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
            "    Loss = 25.5858\n",
            "\n",
            "(w=1.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
            "    Loss = 49.4400\n",
            "\n",
            "(w=1.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
            "    Loss = 45.0000\n",
            "\n",
            "(w=1.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
            "    Loss = 40.8000\n",
            "\n",
            "(w=1.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
            "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
            "    Loss = 36.8400\n",
            "\n",
            "(w=1.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.3, loss: 12.96\n",
            "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
            "    Loss = 33.1200\n",
            "\n",
            "(w=1.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.69\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 4.5, loss: 11.56\n",
            "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
            "    Loss = 29.6400\n",
            "\n",
            "(w=1.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 4.7, loss: 10.24\n",
            "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
            "    Loss = 26.4000\n",
            "\n",
            "(w=1.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
            "    Loss = 23.4000\n",
            "\n",
            "(w=1.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
            "    Loss = 20.6400\n",
            "\n",
            "(w=1.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
            "    Loss = 18.1200\n",
            "\n",
            "(w=1.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
            "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
            "    Loss = 15.8400\n",
            "\n",
            "(w=1.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
            "    Loss = 35.4018\n",
            "\n",
            "(w=1.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
            "    Loss = 31.6298\n",
            "\n",
            "(w=1.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
            "    Loss = 28.0978\n",
            "\n",
            "(w=1.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
            "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
            "    Loss = 24.8058\n",
            "\n",
            "(w=1.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 5.0, loss: 8.41\n",
            "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
            "    Loss = 21.7538\n",
            "\n",
            "(w=1.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.12\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 5.2, loss: 7.29\n",
            "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
            "    Loss = 18.9418\n",
            "\n",
            "(w=1.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 5.4, loss: 6.25\n",
            "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
            "    Loss = 16.3698\n",
            "\n",
            "(w=1.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
            "    Loss = 14.0378\n",
            "\n",
            "(w=1.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
            "    Loss = 11.9458\n",
            "\n",
            "(w=1.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
            "    Loss = 10.0938\n",
            "\n",
            "(w=1.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
            "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
            "    Loss = 8.4818\n",
            "\n",
            "(w=1.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
            "    Loss = 23.7512\n",
            "\n",
            "(w=1.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
            "    Loss = 20.6472\n",
            "\n",
            "(w=1.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
            "    Loss = 17.7832\n",
            "\n",
            "(w=1.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
            "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
            "    Loss = 15.1592\n",
            "\n",
            "(w=1.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 5.7, loss: 4.84\n",
            "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
            "    Loss = 12.7752\n",
            "\n",
            "(w=1.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.67\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 5.9, loss: 4.00\n",
            "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
            "    Loss = 10.6312\n",
            "\n",
            "(w=1.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 6.1, loss: 3.24\n",
            "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
            "    Loss = 8.7272\n",
            "\n",
            "(w=1.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
            "    Loss = 7.0632\n",
            "\n",
            "(w=1.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
            "    Loss = 5.6392\n",
            "\n",
            "(w=1.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
            "    Loss = 4.4552\n",
            "\n",
            "(w=1.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
            "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
            "    Loss = 3.5112\n",
            "\n",
            "(w=1.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
            "    Loss = 14.4882\n",
            "\n",
            "(w=1.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
            "    Loss = 12.0522\n",
            "\n",
            "(w=1.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
            "    Loss = 9.8562\n",
            "\n",
            "(w=1.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
            "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
            "    Loss = 7.9002\n",
            "\n",
            "(w=1.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 6.4, loss: 2.25\n",
            "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
            "    Loss = 6.1842\n",
            "\n",
            "(w=1.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.34\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.6, loss: 1.69\n",
            "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
            "    Loss = 4.7082\n",
            "\n",
            "(w=1.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 6.8, loss: 1.21\n",
            "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
            "    Loss = 3.4722\n",
            "\n",
            "(w=1.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
            "    Loss = 2.4762\n",
            "\n",
            "(w=1.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
            "    Loss = 1.7202\n",
            "\n",
            "(w=1.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
            "    Loss = 1.2042\n",
            "\n",
            "(w=1.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
            "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
            "    Loss = 0.9282\n",
            "\n",
            "(w=1.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
            "    Loss = 7.6128\n",
            "\n",
            "(w=1.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
            "    Loss = 5.8448\n",
            "\n",
            "(w=1.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
            "    Loss = 4.3168\n",
            "\n",
            "(w=1.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
            "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
            "    Loss = 3.0288\n",
            "\n",
            "(w=1.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 7.1, loss: 0.64\n",
            "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
            "    Loss = 1.9808\n",
            "\n",
            "(w=1.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.12\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.3, loss: 0.36\n",
            "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
            "    Loss = 1.1728\n",
            "\n",
            "(w=1.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.5, loss: 0.16\n",
            "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
            "    Loss = 0.6048\n",
            "\n",
            "(w=1.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
            "    Loss = 0.2768\n",
            "\n",
            "(w=1.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
            "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
            "    Loss = 0.1888\n",
            "\n",
            "(w=1.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
            "    Loss = 0.3408\n",
            "\n",
            "(w=1.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
            "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
            "    Loss = 0.7328\n",
            "\n",
            "(w=2.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
            "    Loss = 3.1250\n",
            "\n",
            "(w=2.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
            "    Loss = 2.0250\n",
            "\n",
            "(w=2.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
            "    Loss = 1.1650\n",
            "\n",
            "(w=2.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
            "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
            "    Loss = 0.5450\n",
            "\n",
            "(w=2.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.8, loss: 0.01\n",
            "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
            "    Loss = 0.1650\n",
            "\n",
            "(w=2.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.01\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 8.0, loss: 0.01\n",
            "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
            "    Loss = 0.0250\n",
            "\n",
            "(w=2.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.2, loss: 0.09\n",
            "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
            "    Loss = 0.1250\n",
            "\n",
            "(w=2.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
            "    Loss = 0.4650\n",
            "\n",
            "(w=2.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
            "    Loss = 1.0450\n",
            "\n",
            "(w=2.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
            "    Loss = 1.8650\n",
            "\n",
            "(w=2.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
            "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
            "    Loss = 2.9250\n",
            "\n",
            "(w=2.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
            "    Loss = 1.0248\n",
            "\n",
            "(w=2.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
            "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
            "    Loss = 0.5928\n",
            "\n",
            "(w=2.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
            "    Loss = 0.4008\n",
            "\n",
            "(w=2.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
            "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
            "    Loss = 0.4488\n",
            "\n",
            "(w=2.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.5, loss: 0.36\n",
            "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
            "    Loss = 0.7368\n",
            "\n",
            "(w=2.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.02\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 8.7, loss: 0.64\n",
            "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
            "    Loss = 1.2648\n",
            "\n",
            "(w=2.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 8.9, loss: 1.00\n",
            "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
            "    Loss = 2.0328\n",
            "\n",
            "(w=2.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
            "    Loss = 3.0408\n",
            "\n",
            "(w=2.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
            "    Loss = 4.2888\n",
            "\n",
            "(w=2.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
            "    Loss = 5.7768\n",
            "\n",
            "(w=2.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
            "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
            "    Loss = 7.5048\n",
            "\n",
            "(w=2.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
            "    Loss = 1.3122\n",
            "\n",
            "(w=2.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
            "    Loss = 1.5482\n",
            "\n",
            "(w=2.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
            "    Loss = 2.0242\n",
            "\n",
            "(w=2.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
            "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
            "    Loss = 2.7402\n",
            "\n",
            "(w=2.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 9.2, loss: 1.69\n",
            "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
            "    Loss = 3.6962\n",
            "\n",
            "(w=2.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.14\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 9.4, loss: 2.25\n",
            "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
            "    Loss = 4.8922\n",
            "\n",
            "(w=2.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.6, loss: 2.89\n",
            "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
            "    Loss = 6.3282\n",
            "\n",
            "(w=2.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
            "    Loss = 8.0042\n",
            "\n",
            "(w=2.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
            "    Loss = 9.9202\n",
            "\n",
            "(w=2.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
            "    Loss = 12.0762\n",
            "\n",
            "(w=2.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
            "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
            "    Loss = 14.4722\n",
            "\n",
            "(w=2.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
            "    Loss = 3.9872\n",
            "\n",
            "(w=2.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
            "    Loss = 4.8912\n",
            "\n",
            "(w=2.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
            "    Loss = 6.0352\n",
            "\n",
            "(w=2.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
            "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
            "    Loss = 7.4192\n",
            "\n",
            "(w=2.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.9, loss: 4.00\n",
            "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
            "    Loss = 9.0432\n",
            "\n",
            "(w=2.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.38\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 10.1, loss: 4.84\n",
            "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
            "    Loss = 10.9072\n",
            "\n",
            "(w=2.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.3, loss: 5.76\n",
            "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
            "    Loss = 13.0112\n",
            "\n",
            "(w=2.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
            "    Loss = 15.3552\n",
            "\n",
            "(w=2.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
            "    Loss = 17.9392\n",
            "\n",
            "(w=2.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
            "    Loss = 20.7632\n",
            "\n",
            "(w=2.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
            "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
            "    Loss = 23.8272\n",
            "\n",
            "(w=2.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
            "    Loss = 9.0498\n",
            "\n",
            "(w=2.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
            "    Loss = 10.6218\n",
            "\n",
            "(w=2.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
            "    Loss = 12.4338\n",
            "\n",
            "(w=2.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
            "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
            "    Loss = 14.4858\n",
            "\n",
            "(w=2.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.6, loss: 7.29\n",
            "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
            "    Loss = 16.7778\n",
            "\n",
            "(w=2.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.74\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 10.8, loss: 8.41\n",
            "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
            "    Loss = 19.3098\n",
            "\n",
            "(w=2.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.0, loss: 9.61\n",
            "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
            "    Loss = 22.0818\n",
            "\n",
            "(w=2.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
            "    Loss = 25.0938\n",
            "\n",
            "(w=2.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
            "    Loss = 28.3458\n",
            "\n",
            "(w=2.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
            "    Loss = 31.8378\n",
            "\n",
            "(w=2.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
            "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
            "    Loss = 35.5698\n",
            "\n",
            "(w=3.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
            "    Loss = 16.5000\n",
            "\n",
            "(w=3.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
            "    Loss = 18.7400\n",
            "\n",
            "(w=3.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
            "    Loss = 21.2200\n",
            "\n",
            "(w=3.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
            "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
            "    Loss = 23.9400\n",
            "\n",
            "(w=3.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.3, loss: 11.56\n",
            "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
            "    Loss = 26.9000\n",
            "\n",
            "(w=3.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.21\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 11.5, loss: 12.96\n",
            "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
            "    Loss = 30.1000\n",
            "\n",
            "(w=3.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 11.7, loss: 14.44\n",
            "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
            "    Loss = 33.5400\n",
            "\n",
            "(w=3.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
            "    Loss = 37.2200\n",
            "\n",
            "(w=3.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
            "    Loss = 41.1400\n",
            "\n",
            "(w=3.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
            "    Loss = 45.3000\n",
            "\n",
            "(w=3.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
            "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
            "    Loss = 49.7000\n",
            "\n",
            "(w=3.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
            "    Loss = 26.3378\n",
            "\n",
            "(w=3.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
            "    Loss = 29.2458\n",
            "\n",
            "(w=3.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
            "    Loss = 32.3938\n",
            "\n",
            "(w=3.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
            "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
            "    Loss = 35.7818\n",
            "\n",
            "(w=3.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.0, loss: 16.81\n",
            "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
            "    Loss = 39.4098\n",
            "\n",
            "(w=3.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.80\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.2, loss: 18.49\n",
            "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
            "    Loss = 43.2778\n",
            "\n",
            "(w=3.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.37\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.4, loss: 20.25\n",
            "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
            "    Loss = 47.3858\n",
            "\n",
            "(w=3.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 3.03\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
            "    Loss = 51.7338\n",
            "\n",
            "(w=3.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.76\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
            "    Loss = 56.3218\n",
            "\n",
            "(w=3.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.58\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
            "    Loss = 61.1498\n",
            "\n",
            "(w=3.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.48\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
            "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
            "    Loss = 66.2178\n",
            "\n",
            "(w=3.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
            "    Loss = 38.5632\n",
            "\n",
            "(w=3.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
            "    Loss = 42.1392\n",
            "\n",
            "(w=3.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
            "    Loss = 45.9552\n",
            "\n",
            "(w=3.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
            "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
            "    Loss = 50.0112\n",
            "\n",
            "(w=3.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.7, loss: 23.04\n",
            "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
            "    Loss = 54.3072\n",
            "\n",
            "(w=3.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.50\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.9, loss: 25.00\n",
            "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
            "    Loss = 58.8432\n",
            "\n",
            "(w=3.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.17\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 13.1, loss: 27.04\n",
            "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
            "    Loss = 63.6192\n",
            "\n",
            "(w=3.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 3.92\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
            "    Loss = 68.6352\n",
            "\n",
            "(w=3.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.75\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
            "    Loss = 73.8912\n",
            "\n",
            "(w=3.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.66\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
            "    Loss = 79.3872\n",
            "\n",
            "(w=3.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.66\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
            "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
            "    Loss = 85.1232\n",
            "\n",
            "(w=3.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
            "    Loss = 53.1762\n",
            "\n",
            "(w=3.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
            "    Loss = 57.4202\n",
            "\n",
            "(w=3.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
            "    Loss = 61.9042\n",
            "\n",
            "(w=3.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
            "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
            "    Loss = 66.6282\n",
            "\n",
            "(w=3.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 13.4, loss: 30.25\n",
            "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
            "    Loss = 71.5922\n",
            "\n",
            "(w=3.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.31\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.6, loss: 32.49\n",
            "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
            "    Loss = 76.7962\n",
            "\n",
            "(w=3.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 4.08\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 13.8, loss: 34.81\n",
            "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
            "    Loss = 82.2402\n",
            "\n",
            "(w=3.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.93\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
            "    Loss = 87.9242\n",
            "\n",
            "(w=3.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.86\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
            "    Loss = 93.8482\n",
            "\n",
            "(w=3.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.86\n",
            "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
            "    Loss = 100.0122\n",
            "\n",
            "(w=3.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 6.3, loss: 7.95\n",
            "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
            "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
            "    Loss = 106.4162\n",
            "\n",
            "(w=3.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
            "    Loss = 70.1768\n",
            "\n",
            "(w=3.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
            "    Loss = 75.0888\n",
            "\n",
            "(w=3.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
            "    Loss = 80.2408\n",
            "\n",
            "(w=3.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
            "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
            "    Loss = 85.6328\n",
            "\n",
            "(w=3.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 14.1, loss: 38.44\n",
            "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
            "    Loss = 91.2648\n",
            "\n",
            "(w=3.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.24\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 14.3, loss: 40.96\n",
            "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
            "    Loss = 97.1368\n",
            "\n",
            "(w=3.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.11\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 14.5, loss: 43.56\n",
            "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
            "    Loss = 103.2488\n",
            "\n",
            "(w=3.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.05\n",
            "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 14.7, loss: 46.24\n",
            "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
            "    Loss = 109.6008\n",
            "\n",
            "(w=3.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.08\n",
            "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 14.9, loss: 49.00\n",
            "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
            "    Loss = 116.1928\n",
            "\n",
            "(w=3.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.18\n",
            "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 15.1, loss: 51.84\n",
            "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
            "    Loss = 123.0248\n",
            "\n",
            "(w=3.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.36\n",
            "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 15.3, loss: 54.76\n",
            "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
            "    Loss = 130.0968\n",
            "\n",
            "(w=4.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
            "    Loss = 89.5650\n",
            "\n",
            "(w=4.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
            "    Loss = 95.1450\n",
            "\n",
            "(w=4.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
            "    Loss = 100.9650\n",
            "\n",
            "(w=4.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
            "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
            "    Loss = 107.0250\n",
            "\n",
            "(w=4.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 14.8, loss: 47.61\n",
            "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
            "    Loss = 113.3250\n",
            "\n",
            "(w=4.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.29\n",
            "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 15.0, loss: 50.41\n",
            "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
            "    Loss = 119.8650\n",
            "\n",
            "(w=4.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.25\n",
            "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 15.2, loss: 53.29\n",
            "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 21.2, loss: 106.09\n",
            "    Loss = 126.6450\n",
            "\n",
            "(w=4.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.29\n",
            "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 15.4, loss: 56.25\n",
            "    x: 4.0, y: 9.1, y_hat: 17.4, loss: 68.89\n",
            "    x: 5.0, y: 10.9, y_hat: 21.4, loss: 110.25\n",
            "    Loss = 133.6650\n",
            "\n",
            "(w=4.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.41\n",
            "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 15.6, loss: 59.29\n",
            "    x: 4.0, y: 9.1, y_hat: 17.6, loss: 72.25\n",
            "    x: 5.0, y: 10.9, y_hat: 21.6, loss: 114.49\n",
            "    Loss = 140.9250\n",
            "\n",
            "(w=4.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.61\n",
            "    x: 2.0, y: 5.0, y_hat: 9.8, loss: 23.04\n",
            "    x: 3.5, y: 7.9, y_hat: 15.8, loss: 62.41\n",
            "    x: 4.0, y: 9.1, y_hat: 17.8, loss: 75.69\n",
            "    x: 5.0, y: 10.9, y_hat: 21.8, loss: 118.81\n",
            "    Loss = 148.4250\n",
            "\n",
            "(w=4.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 6.0, loss: 8.41\n",
            "    x: 1.2, y: 3.5, y_hat: 6.8, loss: 10.89\n",
            "    x: 2.0, y: 5.0, y_hat: 10.0, loss: 25.00\n",
            "    x: 3.5, y: 7.9, y_hat: 16.0, loss: 65.61\n",
            "    x: 4.0, y: 9.1, y_hat: 18.0, loss: 79.21\n",
            "    x: 5.0, y: 10.9, y_hat: 22.0, loss: 123.21\n",
            "    Loss = 156.1650\n",
            "\n",
            "BEST:\n",
            "w=2.0, b=1.0, loss=0.0250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdvNc6WQOwhv"
      },
      "source": [
        "Plotting the loss values as a surface graph gives you a picture of the \"optimisation landscape\" for the parameter values. The loss is minimum at $w=2$, $b=1$ (it might be hard to see this clearly from the 3D diagram, but you can trust the numbers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVruAU86OwIA",
        "outputId": "e40940ad-103f-4f2c-941f-0e01264036e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "# enable 3D\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        " # generate combinations of weights and biases\n",
        "(w_list, b_list) = np.meshgrid(weights, biases)\n",
        "\n",
        "# plot loss across weights and bias values\n",
        "surf = ax.plot_surface(w_list.T, b_list.T, loss_matrix,\n",
        "                       linewidth=0, antialiased=False)\n",
        "\n",
        "ax.set_xlabel('w')\n",
        "ax.set_ylabel('b')\n",
        "ax.set_zlabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nWYSrkBcWzs"
      },
      "source": [
        "### Optimisation with derivatives\n",
        "\n",
        "Obviously, the brute force search method above is really computationally expensive (very slow!), especially when there are more parameters and more values to search. A more efficient approach is to start from a random set of parameter values, and then try to \"move down\" the loss graph to the point with a minimum loss. So, in the plot above, you can start at some random $w$ and $b$, and try to move towards the minimum point.\n",
        "\n",
        "The problem is: from where you are, you actually do not know where the minimum point is. You only know that there *is* a minimum point. The question is: how do you know towards which direction you should move (and how far) so that you can get to the minimum point as fast as possible? The answer is downwards (obviously!) and where the slope is steepest.\n",
        "\n",
        "Fortunately, calculus saves us from guessing, and provides us a way to compute the direction of the slope at any point. This is called the *derivative* (or gradient). Intuitively, the derivative $\\frac{\\partial L}{\\partial w}$ tells us by how much $L$ changes when $w$ changes. Similarly, $\\frac{\\partial L}{\\partial b}$ tells us by how much $L$ changes when $b$ changes. So if we move in these directions, we hope that we can ultimately reach a minimum point.\n",
        "\n",
        "If you are well-versed with calculus, you can compute $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ by hand. Otherwise, you can get help from a derivative calculator (e.g. https://www.derivative-calculator.net)\n",
        "\n",
        "As presented in the lectures, the partial derivatives of the loss function with respect to $w$ and to $b$ are:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$\n",
        "\n",
        "Thus, for a *single* point, the partial derivatives are:\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial w} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)}$\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial b} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$\n",
        "\n",
        "Now, complete the `gradient()` method for `SimpleLinearRegression` below to compute the partial derivatives wrt $w$ and $b$ at a given point. To make life easier, just compute and return both $\\frac{\\partial L^{(i)}}{\\partial w}$ and $\\frac{\\partial L^{(i)}}{\\partial b}$ at the same time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBXmI71Ewc9e",
        "outputId": "588f3c3f-4c33-4933-82d5-d095fca4ed76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def gradient(self, x, y):\n",
        "    \"\"\" Compute partial derivatives wrt w and b\n",
        "\n",
        "    Args:\n",
        "        x (float): input instance\n",
        "        y (float): ground truth output\n",
        "\n",
        "    Returns:\n",
        "        tuple: (float, float)\n",
        "            - the first element will be dL/dw\n",
        "            - the second element will be dL/db\n",
        "    \"\"\"\n",
        "    diff = self.forward(x) - y\n",
        "    return diff*x, diff\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.gradient() method\n",
        "SimpleLinearRegression.gradient = gradient\n",
        "\n",
        "\n",
        "## Quick test: This should return (6.0, 3.0)\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2.0\n",
        "y = 5.0\n",
        "(dLdw, dLdb) = model.gradient(x, y)\n",
        "print(dLdw) # should print 6.0\n",
        "print(dLdb) # should print 3.0"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n",
            "3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBw1pRVN-heL"
      },
      "source": [
        "So, in this example, $\\frac{\\partial L}{\\partial w}=6.0$ suggests that when $w$ increases by a very tiny amount, $L$ will increase by 6.0 times that amount.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNctFEY0yiT"
      },
      "source": [
        "#### Gradient descent\n",
        "\n",
        "Now that we have our gradients, let us try to optimise the parameters $w$ and $b$ of our model to minimise the loss. We will use the gradient descent algorithm for this, as discussed in the lectures.\n",
        "\n",
        "You may reimplement the code provided in the lectures, replacing some of the code by reusing the `forward()`, `loss()` and `gradient()` methods of `SimpleLinearRegression` that you implemented earlier. This will help make your code more modular and readable, and help to improve your understanding of gradient descent at a more abstract level.\n",
        "\n",
        "You should be able to obtain $w \\approx 2$ and $b \\approx 1$ by the end of training if you have implemented everything correctly. Also experiment with the learning rate and the number of epochs and observe the effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLdqp4ESCpsR",
        "outputId": "d2e43f7e-afdd-4849-8ba7-7cdf0f090494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    error = 0.0\n",
        "    grad_w = 0.0\n",
        "    grad_b = 0.0\n",
        "    for (x, y) in zip(x_train, y_train):\n",
        "        ### TODO: Complete this\n",
        "        ### 1. Compute the gradients for w and b for this example\n",
        "        (dLdw, dLdb) = model.gradient(x, y)\n",
        "\n",
        "        ### 2. Add the gradients to grad_w and grad_b\n",
        "        grad_w += dLdw\n",
        "        grad_b += dLdb\n",
        "\n",
        "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
        "        error += model.loss(x, y)\n",
        "\n",
        "    # TODO: Update the weights using the (summed) gradients\n",
        "    model.w = model.w - learning_rate*grad_w\n",
        "    model.b = model.b - learning_rate*grad_b\n",
        "\n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\t w: 1.42\t b: 0.37\t L: 269.1845\n",
            "Epoch: 1\t w: 1.87\t b: 0.50\t L: 34.3824\n",
            "Epoch: 2\t w: 2.03\t b: 0.56\t L: 4.7170\n",
            "Epoch: 3\t w: 2.08\t b: 0.58\t L: 0.9615\n",
            "Epoch: 4\t w: 2.10\t b: 0.59\t L: 0.4789\n",
            "Epoch: 5\t w: 2.11\t b: 0.60\t L: 0.4098\n",
            "Epoch: 6\t w: 2.11\t b: 0.61\t L: 0.3931\n",
            "Epoch: 7\t w: 2.11\t b: 0.61\t L: 0.3833\n",
            "Epoch: 8\t w: 2.11\t b: 0.62\t L: 0.3745\n",
            "Epoch: 9\t w: 2.10\t b: 0.63\t L: 0.3661\n",
            "Epoch: 10\t w: 2.10\t b: 0.63\t L: 0.3578\n",
            "Epoch: 11\t w: 2.10\t b: 0.64\t L: 0.3498\n",
            "Epoch: 12\t w: 2.10\t b: 0.64\t L: 0.3419\n",
            "Epoch: 13\t w: 2.10\t b: 0.65\t L: 0.3343\n",
            "Epoch: 14\t w: 2.10\t b: 0.66\t L: 0.3268\n",
            "Epoch: 15\t w: 2.09\t b: 0.66\t L: 0.3195\n",
            "Epoch: 16\t w: 2.09\t b: 0.67\t L: 0.3124\n",
            "Epoch: 17\t w: 2.09\t b: 0.67\t L: 0.3055\n",
            "Epoch: 18\t w: 2.09\t b: 0.68\t L: 0.2987\n",
            "Epoch: 19\t w: 2.09\t b: 0.68\t L: 0.2921\n",
            "Epoch: 20\t w: 2.09\t b: 0.69\t L: 0.2857\n",
            "Epoch: 21\t w: 2.08\t b: 0.69\t L: 0.2794\n",
            "Epoch: 22\t w: 2.08\t b: 0.70\t L: 0.2733\n",
            "Epoch: 23\t w: 2.08\t b: 0.71\t L: 0.2673\n",
            "Epoch: 24\t w: 2.08\t b: 0.71\t L: 0.2615\n",
            "Epoch: 25\t w: 2.08\t b: 0.72\t L: 0.2558\n",
            "Epoch: 26\t w: 2.08\t b: 0.72\t L: 0.2503\n",
            "Epoch: 27\t w: 2.08\t b: 0.73\t L: 0.2448\n",
            "Epoch: 28\t w: 2.07\t b: 0.73\t L: 0.2396\n",
            "Epoch: 29\t w: 2.07\t b: 0.74\t L: 0.2344\n",
            "Epoch: 30\t w: 2.07\t b: 0.74\t L: 0.2294\n",
            "Epoch: 31\t w: 2.07\t b: 0.74\t L: 0.2245\n",
            "Epoch: 32\t w: 2.07\t b: 0.75\t L: 0.2197\n",
            "Epoch: 33\t w: 2.07\t b: 0.75\t L: 0.2150\n",
            "Epoch: 34\t w: 2.07\t b: 0.76\t L: 0.2105\n",
            "Epoch: 35\t w: 2.06\t b: 0.76\t L: 0.2060\n",
            "Epoch: 36\t w: 2.06\t b: 0.77\t L: 0.2017\n",
            "Epoch: 37\t w: 2.06\t b: 0.77\t L: 0.1975\n",
            "Epoch: 38\t w: 2.06\t b: 0.78\t L: 0.1934\n",
            "Epoch: 39\t w: 2.06\t b: 0.78\t L: 0.1893\n",
            "Epoch: 40\t w: 2.06\t b: 0.78\t L: 0.1854\n",
            "Epoch: 41\t w: 2.06\t b: 0.79\t L: 0.1816\n",
            "Epoch: 42\t w: 2.06\t b: 0.79\t L: 0.1779\n",
            "Epoch: 43\t w: 2.05\t b: 0.80\t L: 0.1742\n",
            "Epoch: 44\t w: 2.05\t b: 0.80\t L: 0.1707\n",
            "Epoch: 45\t w: 2.05\t b: 0.81\t L: 0.1672\n",
            "Epoch: 46\t w: 2.05\t b: 0.81\t L: 0.1638\n",
            "Epoch: 47\t w: 2.05\t b: 0.81\t L: 0.1605\n",
            "Epoch: 48\t w: 2.05\t b: 0.82\t L: 0.1573\n",
            "Epoch: 49\t w: 2.05\t b: 0.82\t L: 0.1541\n",
            "Epoch: 50\t w: 2.05\t b: 0.82\t L: 0.1511\n",
            "Epoch: 51\t w: 2.05\t b: 0.83\t L: 0.1481\n",
            "Epoch: 52\t w: 2.04\t b: 0.83\t L: 0.1452\n",
            "Epoch: 53\t w: 2.04\t b: 0.84\t L: 0.1423\n",
            "Epoch: 54\t w: 2.04\t b: 0.84\t L: 0.1396\n",
            "Epoch: 55\t w: 2.04\t b: 0.84\t L: 0.1368\n",
            "Epoch: 56\t w: 2.04\t b: 0.85\t L: 0.1342\n",
            "Epoch: 57\t w: 2.04\t b: 0.85\t L: 0.1316\n",
            "Epoch: 58\t w: 2.04\t b: 0.85\t L: 0.1291\n",
            "Epoch: 59\t w: 2.04\t b: 0.86\t L: 0.1267\n",
            "Epoch: 60\t w: 2.04\t b: 0.86\t L: 0.1243\n",
            "Epoch: 61\t w: 2.04\t b: 0.86\t L: 0.1219\n",
            "Epoch: 62\t w: 2.04\t b: 0.87\t L: 0.1197\n",
            "Epoch: 63\t w: 2.03\t b: 0.87\t L: 0.1174\n",
            "Epoch: 64\t w: 2.03\t b: 0.87\t L: 0.1153\n",
            "Epoch: 65\t w: 2.03\t b: 0.87\t L: 0.1132\n",
            "Epoch: 66\t w: 2.03\t b: 0.88\t L: 0.1111\n",
            "Epoch: 67\t w: 2.03\t b: 0.88\t L: 0.1091\n",
            "Epoch: 68\t w: 2.03\t b: 0.88\t L: 0.1071\n",
            "Epoch: 69\t w: 2.03\t b: 0.89\t L: 0.1052\n",
            "Epoch: 70\t w: 2.03\t b: 0.89\t L: 0.1033\n",
            "Epoch: 71\t w: 2.03\t b: 0.89\t L: 0.1015\n",
            "Epoch: 72\t w: 2.03\t b: 0.90\t L: 0.0997\n",
            "Epoch: 73\t w: 2.03\t b: 0.90\t L: 0.0980\n",
            "Epoch: 74\t w: 2.03\t b: 0.90\t L: 0.0963\n",
            "Epoch: 75\t w: 2.02\t b: 0.90\t L: 0.0947\n",
            "Epoch: 76\t w: 2.02\t b: 0.91\t L: 0.0931\n",
            "Epoch: 77\t w: 2.02\t b: 0.91\t L: 0.0915\n",
            "Epoch: 78\t w: 2.02\t b: 0.91\t L: 0.0900\n",
            "Epoch: 79\t w: 2.02\t b: 0.91\t L: 0.0885\n",
            "Epoch: 80\t w: 2.02\t b: 0.92\t L: 0.0870\n",
            "Epoch: 81\t w: 2.02\t b: 0.92\t L: 0.0856\n",
            "Epoch: 82\t w: 2.02\t b: 0.92\t L: 0.0842\n",
            "Epoch: 83\t w: 2.02\t b: 0.92\t L: 0.0828\n",
            "Epoch: 84\t w: 2.02\t b: 0.93\t L: 0.0815\n",
            "Epoch: 85\t w: 2.02\t b: 0.93\t L: 0.0802\n",
            "Epoch: 86\t w: 2.02\t b: 0.93\t L: 0.0790\n",
            "Epoch: 87\t w: 2.02\t b: 0.93\t L: 0.0777\n",
            "Epoch: 88\t w: 2.01\t b: 0.94\t L: 0.0765\n",
            "Epoch: 89\t w: 2.01\t b: 0.94\t L: 0.0754\n",
            "Epoch: 90\t w: 2.01\t b: 0.94\t L: 0.0742\n",
            "Epoch: 91\t w: 2.01\t b: 0.94\t L: 0.0731\n",
            "Epoch: 92\t w: 2.01\t b: 0.95\t L: 0.0720\n",
            "Epoch: 93\t w: 2.01\t b: 0.95\t L: 0.0710\n",
            "Epoch: 94\t w: 2.01\t b: 0.95\t L: 0.0700\n",
            "Epoch: 95\t w: 2.01\t b: 0.95\t L: 0.0689\n",
            "Epoch: 96\t w: 2.01\t b: 0.95\t L: 0.0680\n",
            "Epoch: 97\t w: 2.01\t b: 0.96\t L: 0.0670\n",
            "Epoch: 98\t w: 2.01\t b: 0.96\t L: 0.0661\n",
            "Epoch: 99\t w: 2.01\t b: 0.96\t L: 0.0652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad3Q827dMohe"
      },
      "source": [
        "### Predictions\n",
        "\n",
        "Now that your model is trained, you can use it to predict some unknown test instances. Complete the code below to predict the output of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTqEAz6GMoRH",
        "outputId": "93857017-9ead-4746-df3d-6529d7c5f273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_predictions = np.zeros((len(y_test),))\n",
        "for (i, x) in enumerate(x_test):\n",
        "    y_predictions[i] = model.forward(x)\n",
        "\n",
        "print(y_predictions)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.98086701 6.98496132 9.99724428]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ANPLrkLVaY"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Finally, let us evaluate the linear regression model you developed. Unlike classification, we will need a different metric for regression. Recall from Lecture 3, a common evaluation metric for regression is the Mean Squared Error (MSE). We will use that for this tutorial.\n",
        "\n",
        "Complete the `mse()` function below to compute the MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fvD2vNXLx6f",
        "outputId": "a5d66000-6aec-4448-e71b-2b3714df045a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def mse(y_gold, y_prediction):\n",
        "    \"\"\" Compute the MSE given the ground truth and predictions\n",
        "\n",
        "    Args:\n",
        "        y_gold (np.ndarray): the correct ground truth values of y\n",
        "        y_prediction (np.ndarray): the predicted values of y\n",
        "\n",
        "    Returns:\n",
        "        float : MSE\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(y_gold) == len(y_prediction)\n",
        "\n",
        "    return np.square(y_gold - y_prediction).mean()\n",
        "\n",
        "\n",
        "# Compute the MSE on model predictions on our toy test data\n",
        "# You should be able to obtain a very small MSE rate\n",
        "print(mse(y_test, y_predictions))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0037169905079314524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6emtY9qqwGSe"
      },
      "source": [
        "## Iris Dataset (Extra exercise)\n",
        "\n",
        "Here is an extra optional exercise for you: try to get your simple linear regression model working on a (slightly) larger and noisier dataset.\n",
        "\n",
        "For this, we will convert the Iris dataset to use as a regression task. More specifically, our task is to predict the *petal width* of a flower (`y`) given the *sepal length* as input (`x`). The code below will give you the dataset in this format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1yTkyHzwnUp",
        "outputId": "976e7982-b2c2-402e-a131-dd3a620edda4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Download iris data if it does not exist\n",
        "if not os.path.exists(\"iris.data\"):\n",
        "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n",
        "def read_dataset_as_regression(filepath):\n",
        "    \"\"\" Read in the dataset from the specified filepath\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The filepath to the dataset file\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x, y), each being a numpy array.\n",
        "               - x is a numpy array with shape (N, ),\n",
        "                   where N is the number of instances\n",
        "               - y is a numpy array with shape (N, ), where each element is a\n",
        "                real-valued float, and N is the number of instances\n",
        "    \"\"\"\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for line in open(filepath):\n",
        "        if line.strip() != \"\": # handle empty rows in file\n",
        "            row = line.strip().split(\",\")\n",
        "            # extract columns 0 as x.\n",
        "            x.append(float(row[0]))\n",
        "\n",
        "            # extract column 3 as y\n",
        "            y.append(float(row[3]))\n",
        "\n",
        "    return (np.array(x), np.array(y))\n",
        "\n",
        "\n",
        "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
        "    \"\"\" Split dataset into training and test sets, according to the given\n",
        "        test set proportion.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Output label, numpy array with shape (N,)\n",
        "        test_proprotion (float): the desired proportion of test examples\n",
        "                                 (0.0-1.0)\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x_train, x_test, y_train, y_test)\n",
        "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
        "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
        "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
        "               - y_test (np.ndarray): Test labels, shape (N_test, )\n",
        "    \"\"\"\n",
        "\n",
        "    shuffled_indices = random_generator.permutation(len(x))\n",
        "    n_test = round(len(x) * test_proportion)\n",
        "    n_train = len(x) - n_test\n",
        "    x_train = x[shuffled_indices[:n_train]]\n",
        "    y_train = y[shuffled_indices[:n_train]]\n",
        "    x_test = x[shuffled_indices[n_train:]]\n",
        "    y_test = y[shuffled_indices[n_train:]]\n",
        "    return (x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "(x, y) = read_dataset_as_regression(\"iris.data\")\n",
        "print(x.shape)  # (150,)\n",
        "print(y.shape)  # (150,)\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "x_train, x_test, y_train, y_test = split_dataset(x, y,\n",
        "                                                 test_proportion=0.2,\n",
        "                                                 random_generator=rg)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150,)\n",
            "(150,)\n",
            "(120,)\n",
            "(30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XM2yXt_0RMO"
      },
      "source": [
        "As usual, it's always a good idea to examine your data before you start:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTF3kSAV0ddn",
        "outputId": "ae91bcc9-0aa5-4574-ded9-141b769e63e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYm-2dB41JYI"
      },
      "source": [
        "### Model\n",
        "\n",
        "You can copy your `SimpleLinearRegression` class from earlier, with the`forward()`, `loss()` and `gradient()` methods that you implemented earlier. This time you can put them all together in the same place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfdsAHaV1IuR"
      },
      "source": [
        "class SimpleLinearRegression:\n",
        "    def __init__(self, random_generator=default_rng()):\n",
        "        self.w = random_generator.standard_normal()\n",
        "        self.b = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w*x + self.b\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        return (self.forward(x) - y)**2\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        diff = self.forward(x) - y\n",
        "        return diff*x, diff"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6zXgIed3gTR"
      },
      "source": [
        "### Optimisation\n",
        "\n",
        "Again, you should not need to change much of your gradient descent implementation from earlier, so just copy it over. You may, however, need to tweak the learning rate and the number of epochs to obtain a reasonable output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Txe_vse3iIH",
        "outputId": "fed1f92c-b562-45e8-f1d3-fd8c883d9fda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    error = 0.0\n",
        "    grad_w = 0.0\n",
        "    grad_b = 0.0\n",
        "    for (x, y) in zip(x_train, y_train):\n",
        "        ### TODO: Complete this\n",
        "        ### 1. Compute the gradients for w and b for this example\n",
        "        (dLdw, dLdb) = model.gradient(x, y)\n",
        "\n",
        "        ### 2. Add the gradients to grad_w and grad_b\n",
        "        grad_w += dLdw\n",
        "        grad_b += dLdb\n",
        "\n",
        "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
        "        error += model.loss(x, y)\n",
        "\n",
        "    # TODO: Update the weights using the (summed) gradients\n",
        "    model.w = model.w - learning_rate*grad_w\n",
        "    model.b = model.b - learning_rate*grad_b\n",
        "\n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\t w: 0.40\t b: -0.02\t L: 477.9991\n",
            "Epoch: 1\t w: 0.33\t b: -0.04\t L: 185.4492\n",
            "Epoch: 2\t w: 0.28\t b: -0.04\t L: 90.8159\n",
            "Epoch: 3\t w: 0.26\t b: -0.05\t L: 60.1997\n",
            "Epoch: 4\t w: 0.24\t b: -0.05\t L: 50.2901\n",
            "Epoch: 5\t w: 0.24\t b: -0.05\t L: 47.0781\n",
            "Epoch: 6\t w: 0.23\t b: -0.06\t L: 46.0325\n",
            "Epoch: 7\t w: 0.23\t b: -0.06\t L: 45.6876\n",
            "Epoch: 8\t w: 0.23\t b: -0.06\t L: 45.5694\n",
            "Epoch: 9\t w: 0.23\t b: -0.06\t L: 45.5246\n",
            "Epoch: 10\t w: 0.23\t b: -0.06\t L: 45.5034\n",
            "Epoch: 11\t w: 0.23\t b: -0.06\t L: 45.4899\n",
            "Epoch: 12\t w: 0.23\t b: -0.06\t L: 45.4789\n",
            "Epoch: 13\t w: 0.23\t b: -0.06\t L: 45.4687\n",
            "Epoch: 14\t w: 0.23\t b: -0.06\t L: 45.4588\n",
            "Epoch: 15\t w: 0.23\t b: -0.06\t L: 45.4489\n",
            "Epoch: 16\t w: 0.23\t b: -0.06\t L: 45.4391\n",
            "Epoch: 17\t w: 0.23\t b: -0.06\t L: 45.4293\n",
            "Epoch: 18\t w: 0.23\t b: -0.07\t L: 45.4195\n",
            "Epoch: 19\t w: 0.23\t b: -0.07\t L: 45.4098\n",
            "Epoch: 20\t w: 0.23\t b: -0.07\t L: 45.4000\n",
            "Epoch: 21\t w: 0.23\t b: -0.07\t L: 45.3902\n",
            "Epoch: 22\t w: 0.23\t b: -0.07\t L: 45.3804\n",
            "Epoch: 23\t w: 0.23\t b: -0.07\t L: 45.3706\n",
            "Epoch: 24\t w: 0.23\t b: -0.07\t L: 45.3609\n",
            "Epoch: 25\t w: 0.23\t b: -0.07\t L: 45.3511\n",
            "Epoch: 26\t w: 0.23\t b: -0.07\t L: 45.3414\n",
            "Epoch: 27\t w: 0.23\t b: -0.07\t L: 45.3316\n",
            "Epoch: 28\t w: 0.23\t b: -0.07\t L: 45.3219\n",
            "Epoch: 29\t w: 0.23\t b: -0.07\t L: 45.3121\n",
            "Epoch: 30\t w: 0.23\t b: -0.07\t L: 45.3024\n",
            "Epoch: 31\t w: 0.23\t b: -0.07\t L: 45.2926\n",
            "Epoch: 32\t w: 0.23\t b: -0.08\t L: 45.2829\n",
            "Epoch: 33\t w: 0.23\t b: -0.08\t L: 45.2732\n",
            "Epoch: 34\t w: 0.23\t b: -0.08\t L: 45.2634\n",
            "Epoch: 35\t w: 0.23\t b: -0.08\t L: 45.2537\n",
            "Epoch: 36\t w: 0.23\t b: -0.08\t L: 45.2440\n",
            "Epoch: 37\t w: 0.23\t b: -0.08\t L: 45.2343\n",
            "Epoch: 38\t w: 0.23\t b: -0.08\t L: 45.2246\n",
            "Epoch: 39\t w: 0.23\t b: -0.08\t L: 45.2149\n",
            "Epoch: 40\t w: 0.23\t b: -0.08\t L: 45.2052\n",
            "Epoch: 41\t w: 0.23\t b: -0.08\t L: 45.1955\n",
            "Epoch: 42\t w: 0.23\t b: -0.08\t L: 45.1858\n",
            "Epoch: 43\t w: 0.23\t b: -0.08\t L: 45.1761\n",
            "Epoch: 44\t w: 0.23\t b: -0.08\t L: 45.1664\n",
            "Epoch: 45\t w: 0.23\t b: -0.08\t L: 45.1567\n",
            "Epoch: 46\t w: 0.23\t b: -0.08\t L: 45.1471\n",
            "Epoch: 47\t w: 0.23\t b: -0.09\t L: 45.1374\n",
            "Epoch: 48\t w: 0.23\t b: -0.09\t L: 45.1277\n",
            "Epoch: 49\t w: 0.23\t b: -0.09\t L: 45.1181\n",
            "Epoch: 50\t w: 0.23\t b: -0.09\t L: 45.1084\n",
            "Epoch: 51\t w: 0.23\t b: -0.09\t L: 45.0988\n",
            "Epoch: 52\t w: 0.23\t b: -0.09\t L: 45.0891\n",
            "Epoch: 53\t w: 0.23\t b: -0.09\t L: 45.0795\n",
            "Epoch: 54\t w: 0.23\t b: -0.09\t L: 45.0698\n",
            "Epoch: 55\t w: 0.23\t b: -0.09\t L: 45.0602\n",
            "Epoch: 56\t w: 0.23\t b: -0.09\t L: 45.0505\n",
            "Epoch: 57\t w: 0.23\t b: -0.09\t L: 45.0409\n",
            "Epoch: 58\t w: 0.23\t b: -0.09\t L: 45.0313\n",
            "Epoch: 59\t w: 0.23\t b: -0.09\t L: 45.0217\n",
            "Epoch: 60\t w: 0.23\t b: -0.09\t L: 45.0121\n",
            "Epoch: 61\t w: 0.23\t b: -0.10\t L: 45.0024\n",
            "Epoch: 62\t w: 0.23\t b: -0.10\t L: 44.9928\n",
            "Epoch: 63\t w: 0.23\t b: -0.10\t L: 44.9832\n",
            "Epoch: 64\t w: 0.23\t b: -0.10\t L: 44.9736\n",
            "Epoch: 65\t w: 0.23\t b: -0.10\t L: 44.9640\n",
            "Epoch: 66\t w: 0.23\t b: -0.10\t L: 44.9544\n",
            "Epoch: 67\t w: 0.23\t b: -0.10\t L: 44.9448\n",
            "Epoch: 68\t w: 0.23\t b: -0.10\t L: 44.9353\n",
            "Epoch: 69\t w: 0.23\t b: -0.10\t L: 44.9257\n",
            "Epoch: 70\t w: 0.23\t b: -0.10\t L: 44.9161\n",
            "Epoch: 71\t w: 0.23\t b: -0.10\t L: 44.9065\n",
            "Epoch: 72\t w: 0.23\t b: -0.10\t L: 44.8970\n",
            "Epoch: 73\t w: 0.23\t b: -0.10\t L: 44.8874\n",
            "Epoch: 74\t w: 0.23\t b: -0.10\t L: 44.8778\n",
            "Epoch: 75\t w: 0.23\t b: -0.10\t L: 44.8683\n",
            "Epoch: 76\t w: 0.23\t b: -0.11\t L: 44.8587\n",
            "Epoch: 77\t w: 0.23\t b: -0.11\t L: 44.8492\n",
            "Epoch: 78\t w: 0.23\t b: -0.11\t L: 44.8396\n",
            "Epoch: 79\t w: 0.23\t b: -0.11\t L: 44.8301\n",
            "Epoch: 80\t w: 0.23\t b: -0.11\t L: 44.8206\n",
            "Epoch: 81\t w: 0.23\t b: -0.11\t L: 44.8110\n",
            "Epoch: 82\t w: 0.23\t b: -0.11\t L: 44.8015\n",
            "Epoch: 83\t w: 0.23\t b: -0.11\t L: 44.7920\n",
            "Epoch: 84\t w: 0.23\t b: -0.11\t L: 44.7824\n",
            "Epoch: 85\t w: 0.23\t b: -0.11\t L: 44.7729\n",
            "Epoch: 86\t w: 0.24\t b: -0.11\t L: 44.7634\n",
            "Epoch: 87\t w: 0.24\t b: -0.11\t L: 44.7539\n",
            "Epoch: 88\t w: 0.24\t b: -0.11\t L: 44.7444\n",
            "Epoch: 89\t w: 0.24\t b: -0.11\t L: 44.7349\n",
            "Epoch: 90\t w: 0.24\t b: -0.11\t L: 44.7254\n",
            "Epoch: 91\t w: 0.24\t b: -0.12\t L: 44.7159\n",
            "Epoch: 92\t w: 0.24\t b: -0.12\t L: 44.7064\n",
            "Epoch: 93\t w: 0.24\t b: -0.12\t L: 44.6969\n",
            "Epoch: 94\t w: 0.24\t b: -0.12\t L: 44.6875\n",
            "Epoch: 95\t w: 0.24\t b: -0.12\t L: 44.6780\n",
            "Epoch: 96\t w: 0.24\t b: -0.12\t L: 44.6685\n",
            "Epoch: 97\t w: 0.24\t b: -0.12\t L: 44.6591\n",
            "Epoch: 98\t w: 0.24\t b: -0.12\t L: 44.6496\n",
            "Epoch: 99\t w: 0.24\t b: -0.12\t L: 44.6401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P7tQX-L31LB"
      },
      "source": [
        "### Visualising your trained model\n",
        "\n",
        "You can visualise your model by plotting the line on the graph.\n",
        "\n",
        "We will also plot the test instances to get a rough idea of how well we expect it to perform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUmFEg7s33Ve",
        "outputId": "653f85b1-e7b5-43d0-d6ae-45ab6b126456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "# Plot training instances\n",
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "\n",
        "# Draw the line representing the model\n",
        "xmin = x_train.min()\n",
        "ymin = model.forward(xmin)\n",
        "xmax = x_train.max()\n",
        "ymax = model.forward(xmax)\n",
        "plt.plot([xmin, xmax], [ymin, ymax], 'r-')\n",
        "\n",
        "# Plot test instances\n",
        "plt.scatter(x_test, y_test, c=\"red\", edgecolor='k')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY37EHKe5JZT"
      },
      "source": [
        "### Predictions and evaluation\n",
        "\n",
        "Finally, predict the test instances given the model.\n",
        "\n",
        "Then evaluate the model with MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHx0gaAr5OfN",
        "outputId": "3bead2f5-9a23-44d3-c4e7-3de75635e3fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_predictions = np.zeros((len(y_test),))\n",
        "for (i, x) in enumerate(x_test):\n",
        "    y_predictions[i] = model.forward(x)\n",
        "\n",
        "print(y_predictions)\n",
        "print(y_test)\n",
        "\n",
        "print(mse(y_test, y_predictions))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.44026853 1.36930649 1.20372841 1.74777067 1.08545836 1.1800744\n",
            " 1.62950062 1.08545836 1.01449632 1.41661451 1.48757655 1.25103643\n",
            " 1.34565248 1.10911237 1.34565248 1.03815033 0.91988028 1.51123056\n",
            " 1.32199847 1.46392254 1.20372841 1.32199847 1.32199847 1.06180434\n",
            " 1.53488457 0.89622626 1.01449632 1.15642039 0.99084231 0.94353429]\n",
            "[1.4 1.8 1.1 2.  0.2 1.1 1.9 0.4 0.1 1.8 2.3 2.4 1.8 0.2 2.3 0.1 0.2 2.3\n",
            " 1.4 1.7 2.  1.2 1.4 1.  1.4 0.1 0.3 0.4 0.2 0.3]\n",
            "0.40058936707735976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Hopefully you have managed to deepen your understanding about linear regression by implementing the model, loss function, and the gradient descent algorithm, and putting everything together for training and testing.\n",
        "\n",
        "In the next lab exercise, we will delve a bit deeper at implementation level, and try to extend your model to handle more than one input variable. We will also start making your code a bit more efficient with vectorised implementations so that you can perform computations on multiple training instances simultaneously. This will hopefully help you get started on implementing Neural Networks (which we will unfortunately not cover in these lab exercises as it is part of your second coursework).  \n",
        "\n"
      ]
    }
  ]
}