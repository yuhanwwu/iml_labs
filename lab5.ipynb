{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuhanwwu/iml_labs/blob/main/lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 5: Multiple Linear & Logistic Regression, and PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-02-08 | Josiah Wang | First version |\n",
        "2021-11-08 | Josiah Wang | Fixed docstring for forward() - should return a np.ndarray. Also fixed some formatting of equations in text for multiple linear regression |\n",
        "2021-11-13 | Josiah Wang | In Multiple linear regression > Gradients, $x_1$ should be $x_1^{(i)}$  |\n",
        "2022-10-28 | Josiah Wang | Added a few more resources for learning PyTorch |\n",
        "2022-11-15 | Josiah Wang | Updated to `ax = fig.add_subplot(projection='3d')`. The original `fig.gca(projection='3d')` is deprecated.\n",
        "2022-11-15 | Josiah Wang | Fixed errors for PyTorch section when running on GPU.\n",
        "2023-11-16 | Josiah Wang | Updated link to the latest version of Python Programming's Deep Learning tuotrial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This lab exercise is divided into three parts.\n",
        "\n",
        "In the first part, you will extend your simple linear regression model from the previous lab to implement a **multiple linear regression** model which can handle more than one input variable. You will also start implementing *vectorised code* to enable your algorithm to perform computations on multiple training instances simultaneously, rather than on one instance at a time.\n",
        "\n",
        "In the second part, you will tweak your linear regression model to become a **logistic regression** classifier to tackle a classification problem. This really only require several tweaks.\n",
        "\n",
        "The final part will consist of a simple introduction to PyTorch, where you will get some idea about how to put together a linear regressor in PyTorch. This will help you figure out how to bring the idea further for implementing Neural Networks.\n",
        "\n",
        "By the end of this lab exercise, you will have\n",
        "- implemented and trained a multiple regression model\n",
        "- implemented vectorised code enabling you to peform batching\n",
        "- implemented and trained a logistic regression model\n",
        "- experimented with using PyTorch for linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ort9kFjO4Hf"
      },
      "source": [
        "## Multiple linear regression\n",
        "\n",
        "In the previous lab exercise, you have implemented a simple linear regression with one input variable and one output variable from scratch.\n",
        "\n",
        "You will often need to use more than one input variable. So let us try to make the linear regression model a bit more general to accomodate more input variables.\n",
        "\n",
        "You will also try to make our computations more efficient by vectorising them to handle multiple data points in one go, rather than individually as you did in the previous lab.  \n",
        "\n",
        "As with the previous exercise, we will develop our model with a toy dataset to ensure that everything is working correctly before using it on a real dataset.\n",
        "\n",
        "In the code below, we generate an artificial dataset with 2 input variables $x_1$ and $x_2$. The dataset is generated from $y=4x_1+2.5x_2+1.5$, so we expect the algorithm to ideally end up with these parameter values once trained. Since the data is two-dimensional, the model is now a *plane* rather than a line. We also add some noise to the output to make life slightly harder for your algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w2N6AZsYJUd"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import default_rng\n",
        "\n",
        "# Generate a toy dataset\n",
        "# There is no need to understand the details,\n",
        "# but I'm generating a random dataset from 4*x_1 + 2.5*x_2 + 1.5\n",
        "# and adding some noise to the output\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "weights = np.array([4, 2.5, 1.5])\n",
        "n_samples = 100\n",
        "x = rg.random((n_samples, 2))*10.0\n",
        "x = np.hstack((x, np.ones((n_samples, 1))))\n",
        "y = np.matmul(x, weights)\n",
        "\n",
        "# add noise to y\n",
        "# comment these out if you want to work with a perfectly clean dataset\n",
        "noise = rg.standard_normal(y.shape)\n",
        "y = y + noise\n",
        "\n",
        "x_train = x[:80, :2]\n",
        "y_train = y[:80]\n",
        "x_test = x[80:, :2]\n",
        "y_test = y[80:]\n",
        "\n",
        "# Plot the training set\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection='3d') # enable 3D\n",
        "ax.scatter(x_train[:,0], x_train[:,1], y_train, c=\"red\")\n",
        "ax.set_xlabel(\"x1\")\n",
        "ax.set_ylabel(\"x2\")\n",
        "ax.set_zlabel('y')\n",
        "\n",
        "# Plot the plane - you are aiming for your algorithm to recover this plane\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection='3d') # enable 3D\n",
        "x_plane = np.linspace(0,10,10)\n",
        "y_plane = np.linspace(0,10,10)\n",
        "x_plane, y_plane = np.meshgrid(x_plane, y_plane)\n",
        "z = weights[0] * x_plane + weights[1] * y_plane + weights[2]\n",
        "surf = ax.plot_surface(x_plane, y_plane, z)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQAZ5dlOU5sZ"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now, we will try to implement a Multiple Linear Regression model that can take more than one input:\n",
        "\n",
        "$$y^{(i)} = w_1 x^{(i)}_1 + w_2 x^{(i)}_2 + w_3 x^{(i)}_3 + \\dotsc + b$$\n",
        "\n",
        "The equation can be made more compact:\n",
        "\n",
        "$$y^{(i)} = \\sum_{k=1}^{K} w_k x^{(i)}_k + b$$\n",
        "\n",
        "where $K$ is the number of features (dimensions/input).\n",
        "\n",
        "To make it even more compact, we can use the \"bias trick\" to combine $b$ with the weights into a single set to save us from having to keep track of the weights and bias parameters separately:\n",
        "\n",
        "$$y^{(i)} = \\sum_{k=1}^{K+1} w_k x^{(i)}_k$$\n",
        "\n",
        "where $w_{K+1}=b$ and $x_{K+1}^{(i)}=1.0$\n",
        "\n",
        "We can also represent this equation in vector notation to make computation more efficient.\n",
        "\n",
        "$$y^{(i)} = W^Tx^{(i)}$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "W =\n",
        "\\begin{bmatrix}\n",
        "w_1\\\\\n",
        "w_2\\\\\n",
        "\\vdots\\\\\n",
        "b\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "x^{(i)} =\n",
        "\\begin{bmatrix}\n",
        "x_1^{(i)}\\\\\n",
        "x_2^{(i)}\\\\\n",
        "\\vdots\\\\\n",
        "1.0\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "and $W^T$ transposes $W$ to be a row vector.\n",
        "\n",
        "Note that the input $x^{(i)}$ includes a constant $1$ to account for the bias term.\n",
        "\n",
        "Going further, we can further improve our representation to allow us to perform computations for multiple data points in one go using matrix multiplication:\n",
        "\n",
        "$$y = XW$$\n",
        "\n",
        "where\n",
        "\n",
        "$$y =\n",
        "\\begin{bmatrix}\n",
        "y^{(1)}\\\\\n",
        "y^{(2)}\\\\\n",
        "\\vdots\\\\\n",
        "y^{(N)}\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$X =\n",
        "\\begin{bmatrix}\n",
        "x_1^{(1)} & x_2^{(1)} & \\dotsb & 1.0\\\\\n",
        "x_1^{(2)} & x_2^{(2)} & \\dotsb & 1.0\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
        "x_1^{(N)} & x_2^{(N)} & \\dotsb & 1.0\\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$$W =\n",
        "\\begin{bmatrix}\n",
        "w_1\\\\\n",
        "w_2\\\\\n",
        "\\vdots\\\\\n",
        "b\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "For convenience, we are now using $y = XW$, otherwise we will have to keep transposing both matrices, i.e. $y = W^TX^T$.\n",
        "\n",
        "Complete the `forward()` method of `MultipleLinearRegression` below. It should really just be a direct translation of the equation $y = XW$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4baUkvjaU_D3"
      },
      "source": [
        "class MultipleLinearRegression:\n",
        "    def __init__(self, n_input_vars, random_generator=default_rng()):\n",
        "        \"\"\" Constructor\n",
        "\n",
        "        Args:\n",
        "            n_input_vars (int): Number of features (including bias)\n",
        "            random_generator (RandomGenerator): A random generator\n",
        "        \"\"\"\n",
        "\n",
        "        # we include the bias as an additional weight here\n",
        "        self.w = random_generator.standard_normal(n_input_vars)\n",
        "        self.w[-1] = 0. # set the bias to 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Perform forward pass given an input x\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (N, K) where\n",
        "                            - N is the number of instances,\n",
        "                            - K is the number of features (including bias)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: the output of the model given the current weights\n",
        "        \"\"\"\n",
        "\n",
        "        ## TODO: Complete this\n",
        "        return ????\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "## Quick test: This should return [17, 15.5]\n",
        "model = MultipleLinearRegression(3)\n",
        "model.w = np.array([4, 2.5, 1.5])\n",
        "x = np.array([[2, 3, 1], [1, 4, 1]])\n",
        "y_hat = model.forward(x)\n",
        "print(y_hat) # should print [17, 15.5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdQWT64100Yj"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "The loss is still the same as what we had in the previous lab. This time, you should return a vector for each of the losses given as input.\n",
        "\n",
        "$$L = \\frac{1}{2} \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \\frac{1}{2} \\sum_{i=1}^{N} \\ell^{(i)} $$\n",
        "\n",
        "$$\\ell =  \n",
        "\\begin{bmatrix}\n",
        "\\ell^{(1)}\\\\\n",
        "\\ell^{(2)}\\\\\n",
        "\\vdots\\\\\n",
        "\\ell^{(N)}\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\left(\\hat{y}^{(1)} - y^{(1)}\\right)^2\\\\\n",
        "\\left(\\hat{y}^{(2)} - y^{(2)}\\right)^2\\\\\n",
        "\\vdots\\\\\n",
        "\\left(\\hat{y}^{(N)} - y^{(N)}\\right)^2\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Complete the `loss()` method for `MultipleLinearRegresion` below. **Tip**: you should not need to change anything from the previous lab!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20OuZPLx0ykL"
      },
      "source": [
        "# Loss method for MultipleLinearRegression\n",
        "def loss(self, x, y):\n",
        "    \"\"\" Compute the loss for an input x\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): shape (N, K) where\n",
        "                        - N is the number of instances,\n",
        "                        - K is the number of features (including bias)\n",
        "        y (np.ndarray): shape (N, ), the ground truth output labels for\n",
        "                        each of the instance in x\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: shape (N,), the output of the model given the current weights\n",
        "                    for each instance in x\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Complete this (it's the same as in the previous lab)\n",
        "    return ????\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the MultipleLinearRegression.loss() method\n",
        "MultipleLinearRegression.loss = loss\n",
        "\n",
        "\n",
        "## Quick test: This should return [1, 2.25]\n",
        "model = MultipleLinearRegression(3)\n",
        "model.w = np.array([4, 2.5, 1.5])\n",
        "x = np.array([[2, 3, 1], [1, 4, 1]])\n",
        "y = np.array([18, 14])\n",
        "test_loss = model.loss(x, y)\n",
        "print(test_loss) # should print [1, 2.25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaOVjsFU5aar"
      },
      "source": [
        "### Gradients\n",
        "\n",
        "Similarly, for each instance, you will need to return the gradients wrt each parameter.\n",
        "\n",
        "For your convenience, the gradients are follows:\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial w_1} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x_1^{(i)}$\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial w_2} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x_2^{(i)}$\n",
        "\n",
        "$\\dotsc$\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial b} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$\n",
        "\n",
        "You should return\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial L^{(1)}}{\\partial W}\\\\\n",
        "\\frac{\\partial L^{(2)}}{\\partial W}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial L^{(N)}}{\\partial W}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial L^{(1)}}{\\partial w_1} & \\frac{\\partial L^{(1)}}{\\partial w_2} & \\dotsc & \\frac{\\partial L^{(1)}}{\\partial b}\\\\\n",
        "\\frac{\\partial L^{(2)}}{\\partial w_1} & \\frac{\\partial L^{(2)}}{\\partial w_2} & \\dotsc & \\frac{\\partial L^{(2)}}{\\partial b}\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
        "\\frac{\\partial L^{(N)}}{\\partial w_1} & \\frac{\\partial L^{(N)}}{\\partial w_2} & \\dotsc & \\frac{\\partial L^{(N)}}{\\partial b}\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Complete the `gradient()` method of `MultipleLinearRegression` below. Tip: take advantage of the repetitive patterns between the derivatives for the different parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz_7b3wZ-mk-"
      },
      "source": [
        "def gradient(self, x, y):\n",
        "    \"\"\" Compute partial derivatives wrt w and b\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): shape (N, K) where\n",
        "                        - N is the number of instances,\n",
        "                        - K is the number of features (including bias)\n",
        "        y (np.ndarray): shape (N, ), the ground truth output labels for\n",
        "                        each of the instance in x\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: shape (N, K) containing the partial derivatives\n",
        "                        wrt the K weights, for each N instance\n",
        "    \"\"\"\n",
        "    ## TODO: Complete this\n",
        "    return ????\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.gradient() method\n",
        "MultipleLinearRegression.gradient = gradient\n",
        "\n",
        "\n",
        "## Quick test\n",
        "model = MultipleLinearRegression(3)\n",
        "model.w = np.array([4, 2.5, 1.5])\n",
        "x = np.array([[2, 3, 1], [1, 4, 1]])\n",
        "y = np.array([18, 14])\n",
        "grad = model.gradient(x, y)\n",
        "print(grad)\n",
        "# should print\n",
        "#[[-2.  -3.  -1. ]\n",
        "# [ 1.5  6.   1.5]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biXL47WEWf3U"
      },
      "source": [
        "### Optimisation with gradient descent\n",
        "\n",
        "Finally, it is time to train your model! We will use vanilla gradient descent as in the last exercise.\n",
        "\n",
        "Complete the code below. The main change you will need to make compared to last time is that you can now compute the gradients for all training instances in one go, without having to iterate over each individually.\n",
        "\n",
        "Play around with the learning rate and the number of epochs. You will definitely need to adjust the learning rate! You should be able to approximately recover the parameters of our toy dataset ($w_1=4, w_2=2.5, b=1.5$). You can also try generating a clean version of the dataset (without the added noise) and you will observe a much lower loss.\n",
        "\n",
        "Once you are done, feel free to try implementing stohastic or mini-batched gradient descent if you feel like it (completely optional)!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYIY03v_W_kN"
      },
      "source": [
        "n_input = 2\n",
        "model = MultipleLinearRegression(n_input+1, rg)\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 1000\n",
        "\n",
        "# concat the constant 1 to each instance for the bias\n",
        "x_train_ext = np.hstack((x_train, np.ones((x_train.shape[0], 1))))\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # TODO: compute the overall loss for x_train_ext\n",
        "    error = ????\n",
        "\n",
        "    # TODO: compute the sum of the gradients across instances wrt to each parameter\n",
        "    grad = ????\n",
        "\n",
        "    model.w = model.w - learning_rate * grad\n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Blqkd_qhY7D"
      },
      "source": [
        "### Prediction and evaluation\n",
        "\n",
        "For completeness, let us just make some predictions on the test set, and compute the MSE on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ0q0uBWhXiG"
      },
      "source": [
        "def mse(y_gold, y_prediction):\n",
        "    assert len(y_gold) == len(y_prediction)\n",
        "    return np.square(y_gold - y_prediction).mean()\n",
        "\n",
        "\n",
        "# concat the constant 1 to each test instance for the bias\n",
        "x_test_ext = np.hstack((x_test, np.ones((x_test.shape[0], 1))))\n",
        "\n",
        "# predict the output for x_test_ext\n",
        "y_predictions = model.forward(x_test_ext)\n",
        "\n",
        "print(f\"Gold, Prediction\")\n",
        "for (gold, prediction) in zip(y_test, y_predictions):\n",
        "    print(f\"{gold:.4f}, {prediction:.4f}\")\n",
        "\n",
        "print(\"MSE: \", mse(y_test, y_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsmG062_sWOm"
      },
      "source": [
        "## Logistic regression\n",
        "\n",
        "The linear regression model can be easily extended to perform classification. In this section, you will now turn the linear regression model from earlier into a **logistic regression** model that can be used to perform **binary classification**.\n",
        "\n",
        "We will go back to our good old Iris dataset for this section. To enable us to perform *binary* classification, we will only keep the first 100 instances (which make up the first two classes).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9zBL-m6vO0y"
      },
      "source": [
        "import os\n",
        "\n",
        "# Download iris data if it does not exist\n",
        "if not os.path.exists(\"iris.data\"):\n",
        "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n",
        "def read_dataset(filepath):\n",
        "    \"\"\" Read in the dataset from the specified filepath\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The filepath to the dataset file\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x, y, classes), each being a numpy array.\n",
        "               - x is a numpy array with shape (N, K),\n",
        "                   where N is the number of instances\n",
        "                   K is the number of features/attributes\n",
        "               - y is a numpy array with shape (N, ), and each element should be\n",
        "                   an integer from 0 to C-1 where C is the number of classes\n",
        "               - classes : a numpy array with shape (C, ), which contains the\n",
        "                   unique class labels corresponding to the integers in y\n",
        "    \"\"\"\n",
        "\n",
        "    x = []\n",
        "    y_labels = []\n",
        "    for line in open(filepath):\n",
        "        if line.strip() != \"\": # handle empty rows in file\n",
        "            row = line.strip().split(\",\")\n",
        "            x.append(list(map(float, row[:-1])))\n",
        "            y_labels.append(row[-1])\n",
        "\n",
        "    x = x[:100]\n",
        "    y_labels = y_labels[:100]\n",
        "\n",
        "    [classes, y] = np.unique(y_labels, return_inverse=True)\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return (x, y, classes)\n",
        "\n",
        "\n",
        "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
        "    \"\"\" Split dataset into training and test sets, according to the given\n",
        "        test set proportion.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Output label, numpy array with shape (N,)\n",
        "        test_proprotion (float): the desired proportion of test examples\n",
        "                                 (0.0-1.0)\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x_train, x_test, y_train, y_test)\n",
        "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
        "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
        "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
        "               - y_test (np.ndarray): Test labels, shape (N_test, )\n",
        "    \"\"\"\n",
        "\n",
        "    shuffled_indices = random_generator.permutation(len(x))\n",
        "    n_test = round(len(x) * test_proportion)\n",
        "    n_train = len(x) - n_test\n",
        "    x_train = x[shuffled_indices[:n_train]]\n",
        "    y_train = y[shuffled_indices[:n_train]]\n",
        "    x_test = x[shuffled_indices[n_train:]]\n",
        "    y_test = y[shuffled_indices[n_train:]]\n",
        "    return (x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "(x, y, classes) = read_dataset(\"iris.data\")\n",
        "print(x.shape)  # (100, 4)\n",
        "print(y.shape)  # (100,)\n",
        "print(classes)  # ['Iris-setosa' 'Iris-versicolor']\n",
        "\n",
        "x_train, x_test, y_train, y_test = split_dataset(x, y,\n",
        "                                                 test_proportion=0.2,\n",
        "                                                 random_generator=rg)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5xizBFE8UsJ"
      },
      "source": [
        "We will also perform standardisation on the dataset to ensure that the features are in a similar range. This is important because the range of the features is quite different, and this will affect parameter optimisation using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w7kmCpS8UWD"
      },
      "source": [
        "# perform standardisation on the dataset\n",
        "mu = x_train.mean(axis=0)\n",
        "sigma = x_train.std(axis=0)\n",
        "\n",
        "x_train = (x_train - mu) / sigma\n",
        "x_test = (x_test - mu) / sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQnH6ih11pJu"
      },
      "source": [
        "### Model\n",
        "\n",
        "You will now adapt the linear regression model from earlier and turn it into a logistic regression classifier.\n",
        "\n",
        "The Logistic Regression classifier should return:\n",
        "\n",
        "$$y = \\sigma(XW)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$ \\sigma(x) = \\frac{1}{1 + exp^{-x}}$$\n",
        "\n",
        "Firstly, you need to modify the `forward()` method. This is really just a tiny modification from the linear regression model from earlier. For your convenience, I have included a `sigmoid()` function for your use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajak-bU-sZ1B"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, n_input_vars, random_generator=default_rng()):\n",
        "        \"\"\" Constructor\n",
        "\n",
        "        Args:\n",
        "            n_input_vars (int): Number of features (including bias)\n",
        "            random_generator (RandomGenerator): A random generator\n",
        "        \"\"\"\n",
        "\n",
        "        # we include the bias as an additional weight here\n",
        "        self.w = random_generator.standard_normal(n_input_vars)\n",
        "        self.w[-1] = 0. # set the bias to 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Perform forward pass given an input x\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray): shape (N, K) where\n",
        "                            - N is the number of instances,\n",
        "                            - K is the number of features (including bias)\n",
        "\n",
        "        Returns:\n",
        "            np.npdarray: the output of the model given the current weights\n",
        "        \"\"\"\n",
        "        ## TODO: Complete this\n",
        "        return ????\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "## Quick test\n",
        "model = LogisticRegression(3)\n",
        "model.w = np.array([2, 1, 0.5])\n",
        "x = np.array([[0.3, 0.5, 0.4], [-0.6, -0.6, -0.5]])\n",
        "y_hat = model.forward(x)\n",
        "print(y_hat) # should print [0.78583498 0.11405238]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekeR1vgZ5a_H"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "Logistic regression uses a different loss function (binary cross-entropy), so this needs to be modified.\n",
        "\n",
        "$$L = -\\sum_{i=1}^N (y^{(i)}log(\\hat{y}^{(i)}) + (1 - y^{(i)})log(1 - \\hat{y}^{(i)}))$$\n",
        "\n",
        "Also, rather than returning the loss for each individual instance, this time we will modify `loss()` so that it returns the overall loss, i.e. $L$ in the equation above. So, `loss()` should return a single float. This is more for our own convenience, since we only need the overall loss anyway.\n",
        "\n",
        "We will use the `np.log()` (natural logarithm) for this exercise, although it is actually more common to use $log_2$. It is fine to use either as long as you are consistent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3xp36lt5gFX"
      },
      "source": [
        "def loss(self, x, y):\n",
        "    \"\"\" Compute the loss for an input x\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): shape (N, K) where\n",
        "                        - N is the number of instances,\n",
        "                        - K is the number of features (including bias)\n",
        "        y (np.ndarray): shape (N, ), the ground truth output labels for\n",
        "                        each of the instance in x\n",
        "\n",
        "    Returns:\n",
        "        float: the overall loss\n",
        "    \"\"\"\n",
        "    ### TODO: Complete this\n",
        "    return ????\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the LogisticRegression.loss() method\n",
        "LogisticRegression.loss = loss\n",
        "\n",
        "\n",
        "## Quick test\n",
        "model = LogisticRegression(3)\n",
        "model.w = np.array([2, 1, 0.5])\n",
        "x = np.array([[0.3, 0.5, 0.4], [-0.6, -0.6, -0.5]])\n",
        "y = np.array([1, 0])\n",
        "test_loss = model.loss(x, y)\n",
        "print(test_loss) # should print 0.3621 if you are using np.log (natural logarithm)\n",
        "\n",
        "# as a reference, the values for the individual loss are [-0.24100845 -0.12109745]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vPqjBfG5z8d"
      },
      "source": [
        "### Gradients\n",
        "\n",
        "The gradients remain the same as Linear Regression, so no modification is needed.\n",
        "\n",
        "We will, however, slightly modify `gradient()` so that we return the *sum* of the partial derivatives across all instances. Therefore, the output should be a vector, where each element is the partial derivative wrt to a weight. This should only be a very minor modification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPsJ4BB05y54"
      },
      "source": [
        "def gradient(self, x, y):\n",
        "    \"\"\" Compute partial derivatives wrt w and b\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): shape (N, K) where\n",
        "                        - N is the number of instances,\n",
        "                        - K is the number of features (including bias)\n",
        "        y (np.ndarray): shape (N, ), the ground truth output labels for\n",
        "                        each of the instance in x\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: shape (K,) containing the sum of the partial derivatives\n",
        "                        wrt the K weights\n",
        "    \"\"\"\n",
        "    ## TODO: Complete this\n",
        "    ## (a small change from the previous solution to sum up the gradients)\n",
        "    return ????\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.gradient() method\n",
        "LogisticRegression.gradient = gradient\n",
        "\n",
        "\n",
        "## Quick test\n",
        "model = LogisticRegression(3)\n",
        "model.w = np.array([2, 1, 0.5])\n",
        "x = np.array([[0.3, 0.5, 0.4], [-0.6, -0.6, -0.5]])\n",
        "y = np.array([1, 0])\n",
        "grad = model.gradient(x, y)\n",
        "print(grad) # should print [-0.13268093 -0.17551394 -0.1426922 ]\n",
        "\n",
        "# as a reference, the values of the individual gradients are\n",
        "# [[-0.06424951 -0.10708251 -0.08566601]\n",
        "#  [-0.06843143 -0.06843143 -0.05702619]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blPks3GoDXDP"
      },
      "source": [
        "### Optimisation with gradient descent\n",
        "\n",
        "Gradient descent should not need any major modification.\n",
        "\n",
        "There is now no need to sum the loss and gradients since we have already done that in `loss()` and `gradients()`.\n",
        "\n",
        "As usual, play around with the learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ9Av-3Xx5cq"
      },
      "source": [
        "n_input = x_train.shape[1]\n",
        "model = LogisticRegression(n_input+1, rg)\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 1000\n",
        "\n",
        "# concat the constant 1 to each instance for the bias\n",
        "x_train_ext = np.hstack((x_train, np.ones((x_train.shape[0], 1))))\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    error = model.loss(x_train_ext, y_train)\n",
        "    grad = model.gradient(x_train_ext, y_train)\n",
        "    model.w = model.w - learning_rate * grad\n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zheJYfThFkz5"
      },
      "source": [
        "### Prediction and evaluation\n",
        "\n",
        "Finally, we will predict the class label on the test set, and use our good old `accuracy` metric for evaluation.\n",
        "\n",
        "The `forward()` method of `LogisticRegression` still returns floating point values. You will still need to predict a discrete class label (`0` or `1`) for each instance based on this value.\n",
        "\n",
        "This can easily be done with simple thresholding:\n",
        "- if $\\hat{y} >= 0.5$, classify as class 1\n",
        "- otherwise classify as class 0\n",
        "\n",
        "Complete the code below to do this.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qAovhxhyRb8"
      },
      "source": [
        "def accuracy(y_gold, y_prediction):\n",
        "    assert len(y_gold) == len(y_prediction)\n",
        "\n",
        "    try:\n",
        "        return np.sum(y_gold == y_prediction) / len(y_gold)\n",
        "    except ZeroDivisionError:\n",
        "        return 0\n",
        "\n",
        "# concat the constant 1 to each test instance for the bias\n",
        "x_test_ext = np.hstack((x_test, np.ones((x_test.shape[0], 1))))\n",
        "\n",
        "# predict the output for x_test_ext\n",
        "y_predictions = model.forward(x_test_ext)\n",
        "\n",
        "### TODO: Convert the float y_predictions to return either 0 or 1\n",
        "????\n",
        "\n",
        "print(f\"Gold, Prediction\")\n",
        "for (gold, prediction) in zip(y_test, y_predictions):\n",
        "    print(f\"{gold:.4f}, {prediction:.4f}\")\n",
        "\n",
        "print(\"Accuracy: \", accuracy(y_test, y_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uq-MI_UGAM3"
      },
      "source": [
        "## Linear regression with PyTorch\n",
        "\n",
        "This final section is a practical introduction to using PyTorch, a library which you might find useful for building Neural Networks, and for your second coursework (if you plan to use it).\n",
        "\n",
        "PyTorch provides you with the `autograd` library, which magically compute gradients for you automatically, removing the need for you to manually work out and implement gradient computations by hand (as we have been doing). This will especially be a bigger hassle when you deal with neural networks.\n",
        "\n",
        "It also provides you with all the basic \"building blocks\" to build your neural network layers, as well as loss functions and optimisers (like gradient descent). So, it basically automates everything you have constructed by hand so far (and much more!)\n",
        "\n",
        "While PyTorch is mainly used for building neural networks, we will introduce it here for linear regression as an example. In reality, it is a bit of an overkill! But you deserve a break since you have worked hard building everything from scratch so far! 😊\n",
        "\n",
        "I will provide all the codes for this section, so you should just spend time trying to understand how it all works.\n",
        "\n",
        "Now, run the following code to get our toy dataset from earlier up again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqLFXftVS3mr"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import default_rng\n",
        "\n",
        "# Generate a toy dataset\n",
        "# There is no need to understand the details,\n",
        "# but I'm generating a random dataset from 4*x_1 + 2.5*x_2 + 1.5\n",
        "# and adding some noise to the output\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "weights = np.array([4, 2.5, 1.5])\n",
        "n_samples = 100\n",
        "x = rg.random((n_samples, 2))*10.0\n",
        "x = np.hstack((x, np.ones((n_samples, 1))))\n",
        "y = np.matmul(x, weights)\n",
        "\n",
        "# add noise to y\n",
        "# comment this out if you want to work with a perfectly clean dataset\n",
        "noise = rg.standard_normal(y.shape)\n",
        "y = y + noise\n",
        "\n",
        "x_train = np.array(x[:80, :2])\n",
        "y_train = y[:80, np.newaxis]\n",
        "x_test = x[80:, :2]\n",
        "y_test = y[80:, np.newaxis]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bAI_1JtU-L8"
      },
      "source": [
        "### PyTorch Tensors\n",
        "\n",
        "A PyTorch `Tensor` is essentially a NumPy array. The additional features are that:\n",
        "- it allows you to store the value of the gradients from the automatic computation;\n",
        "- it allows you to use GPUs for computation.\n",
        "\n",
        "You can construct a new tensor using `torch.tensor()`, passing your list/NumPy array as the first argument. If you already have a NumPy array and do not wish to make a copy, use `torch.from_numpy()`. This way, any changes you make to the tensor will reflect in the original NumPy array.\n",
        "\n",
        "Let's convert our toy dataset into PyTorch tensors. Run the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PanjTaOyRLL1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "x_test_tensor = torch.from_numpy(x_test).float()\n",
        "y_test_tensor = torch.from_numpy(y_test).float()\n",
        "\n",
        "# Ask PyTorch to store any computed gradients so that we can examine them\n",
        "x_train_tensor.requires_grad_(True)\n",
        "\n",
        "# should be \"None\" at the moment. It will only be filled later after you call backward()\n",
        "print(x_train_tensor.grad)\n",
        "\n",
        "# Use a GPU if it exists\n",
        "# On Google Colab, go to \"Edit > Notebook Settings\" to allocate a GPU\n",
        "if torch.cuda.is_available():\n",
        "    x_train_tensor = x_train_tensor.to('cuda')\n",
        "    x_test_tensor = x_test_tensor.to('cuda')\n",
        "    y_train_tensor = y_train_tensor.to('cuda')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J99XDNUHTJUl"
      },
      "source": [
        "### Construct a model\n",
        "\n",
        "You can construct a model by subclassing `torch.nn.Module`. You will need to implement the `forward()` method just like what you did earlier. This time, instead of implementing the equation yourself, you can just plug in PyTorch's `nn.Linear` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtMN7EiXZW9L"
      },
      "source": [
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, n_input_vars, n_output_vars=1):\n",
        "        super().__init__() # call constructor of superclass\n",
        "        self.linear = nn.Linear(n_input_vars, n_output_vars)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "model = LinearRegression(n_input_vars=2)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')\n",
        "\n",
        "print(model.linear.weight)\n",
        "print(model.linear.bias)\n",
        "print(list(model.parameters()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCko0CDydn3B"
      },
      "source": [
        "### Select loss function\n",
        "\n",
        "Next is the loss function. Choose what you need from PyTorch's library, all ready to use off the shelf! https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\n",
        "We'll use MSE as the loss this time. If you need sum-of-squared error (like what we have been previously using), supply`reduction=\"sum\"` as an argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_tQ5PdGethh"
      },
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "#criterion = torch.nn.MSELoss(reduction=\"sum\") # for SSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBzCUsDffkTz"
      },
      "source": [
        "### Select optimiser\n",
        "\n",
        "Now, choose your favourite optimiser from PyTorch's ready-made optimisers:\n",
        "https://pytorch.org/docs/stable/optim.html#algorithms\n",
        "\n",
        "We'll stick to stohastic gradient descent (SGD) for this exercise. For this optimiser, we can set the learning rate with `lr`. Also remember to pass in the parameters that you need optimising."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxEj4a_Hf-PI"
      },
      "source": [
        "optimiser = torch.optim.SGD(model.parameters(), lr=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s53rP9M7hQEB"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Now, we can train our model. The steps inside the training loop is pretty standard. You should be able to understand the steps easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt38Ns6yhYRh"
      },
      "source": [
        "n_epochs = 5000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Reset the gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    y_hat = model(x_train_tensor)\n",
        "\n",
        "    # compute loss\n",
        "    loss = criterion(y_hat, y_train_tensor)\n",
        "\n",
        "    # Backward pass (compute the gradients)\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimiser.step()\n",
        "\n",
        "    print(f\"Epoch: {epoch}\\t w: {model.linear.weight.data[0]}\\t b: {model.linear.bias.data[0]:.4f} \\t L: {loss:.4f}\")\n",
        "\n",
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJTl2V9Hqo5S"
      },
      "source": [
        "### Prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6h9J5U5qxbm"
      },
      "source": [
        "def mse(y_gold, y_prediction):\n",
        "    assert len(y_gold) == len(y_prediction)\n",
        "    return np.square(y_gold - y_prediction).mean()\n",
        "\n",
        "y_predictions = model.forward(x_test_tensor)\n",
        "\n",
        "print(f\"Gold, Prediction\")\n",
        "for (gold, prediction) in zip(y_test_tensor, y_predictions):\n",
        "    print(f\"{gold.data[0]}, {prediction.data[0]}\")\n",
        "\n",
        "print(\"MSE: \", mse(y_test_tensor, y_predictions.detach().cpu().numpy()).data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzR2Rm89uaap"
      },
      "source": [
        "And that is it for our introduction to PyTorch. Essentially, you have implemented the equivalent of a single neuron with an identity activation function. To construct a multi-layered Neural Network, you will have to assemble and nest the \"building blocks\" provided by PyTorch in a systematic fashion, and choose suitable activation functions. Otherwise, the optimisation process will be similar.\n",
        "\n",
        "There are many resources if you need further help with PyTorch. The [official PyTorch tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) is useful for a quick tutorial.\n",
        "\n",
        "The [Deep Learning module](http://wp.doc.ic.ac.uk/bkainz/teaching/70010-deep-learning) also has two nice tutorials (under Week 2), prepared by one of their (and our former) Teaching Assistants Daniel Pace. If you are in a hurry, skip to 26:25 for the first video since most topics before that are NumPy and Tensor basics that you have already used. You only need to know that the tutorial uses underscores after a variable (e.g. `a_`) to indicate that it is a Tensor.\n",
        "\n",
        "Alternatively, my Teaching Assistant Luca Grillotti has also prepared a [Deep Learning with PyTorch](https://python.pages.doc.ic.ac.uk/lessons/pytorch/) tutorial for my Python Programming module. It goes more deeply into PyTorch. You can skip to Chapter 7 to build an image classifier if you are in a hurry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You have constructed a multiple linear regressor and logistic regression classifier from scratch, and also implemented batching for more efficient computation.\n",
        "\n",
        "You have also been introduced to PyTorch, by using PyTorch to implement linear regression. Hopefully the hard work you did with implementing the model from scratch in earlier sections paid off, and actually helped you understand and appreciate the power of PyTorch better!\n",
        "\n",
        "We will leave the task of implementing Neural Networks for your coursework, but hopefully these lab exercises will help start you off in the correct direction!\n",
        "\n"
      ]
    }
  ]
}